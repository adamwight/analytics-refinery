#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Note: You should make sure to put refinery/python on your PYTHONPATH.
#   export PYTHONPATH=$PYTHONPATH:/path/to/refinery/python

"""
Automatically deletes old _SUCCESS files and cleans the corresponding
directory tree from the banner activity folder in HDFS.
Note that this folder does not store any data, only _SUCCESS files
that are used to coordinate oozie jobs.

Usage: refinery-drop-banner-activity-partitions [options]

Options:
    -h --help                           Show this help message and exit.
    -d --older-than-days=<days>         Drop folders older than this number of days.  [default: 90]
    -l --location=<location>            Base HDFS location path of the banner activity data.
    -n --dry-run                        Don't actually delete any data, but print the HDFS paths
                                        that will be deleted.
"""

from docopt import docopt
from datetime import datetime
from dateutil import relativedelta
from refinery.util import HdfsUtils
import logging
import sys
import os


if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)
    days            = int(arguments['--older-than-days'])
    location        = arguments['--location']
    dry_run         = arguments['--dry-run']

    # configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)-6s %(message)s',
        datefmt='%Y-%m-%dT%H:%M:%S'
    )

    # files or folders older than this date should be deleted
    threshold_datetime = datetime.now() - relativedelta.relativedelta(days=days)

    # list of paths marked for deletion
    paths_to_delete = []

    # iterate year folders
    year_glob = os.path.join(location, 'daily/*')
    for year_path in HdfsUtils.ls(year_glob, include_children=False):
        year = int(year_path[year_path.rindex('=') + 1:])

        # check if whole year can be deleted
        end_of_year_datetime = datetime(year, 1, 1) + relativedelta.relativedelta(years=1)
        if end_of_year_datetime < threshold_datetime:
            # mark whole year for deletion
            paths_to_delete.append(year_path)
        else:
            # iterate month folders
            month_glob = os.path.join(location, 'daily/year=%d/*' % year)
            for month_path in HdfsUtils.ls(month_glob, include_children=False):
                month = int(month_path[month_path.rindex('=') + 1:])

                # check if whole month can be deleted
                end_of_month_datetime = datetime(year, month, 1) + relativedelta.relativedelta(months=1)
                if end_of_month_datetime < threshold_datetime:
                    # mark whole month for deletion
                    paths_to_delete.append(month_path)
                else:
                    # iterate day folders
                    day_glob = os.path.join(location, 'daily/year=%d/month=%d/*' % (year, month))
                    for day_path in HdfsUtils.ls(day_glob, include_children=False):
                        day = int(day_path[day_path.rindex('=') + 1:])

                        # check if day can be deleted
                        end_of_day_datetime = datetime(year, month, day) + relativedelta.relativedelta(days=1)
                        if end_of_day_datetime < threshold_datetime:
                            paths_to_delete.append(day_path)

    if dry_run:
        logging.info('Listing {0} directories from {1} ...'.format(len(paths_to_delete), location))
        for path_to_delete in paths_to_delete:
            print path_to_delete
        logging.info('Dry run finished!')
    else:
        logging.info('Removing {0} directories from {1} ...'.format(len(paths_to_delete), location))
        HdfsUtils.rm(' '.join(paths_to_delete))
        logging.info('Done!')
