#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Note: needs python3 to run unless we backport concurrent and urllib.parse
#
# Will be scheduled on a cron, every 7 days, as:
#   python3 sqoop-mediawiki-tables \
#     --jdbc-host analytics-store.eqiad.wmnet \
#     --output-dir /wmf/data/raw/mediawiki/tables \
#     --wiki-file \
# "/mnt/hdfs/wmf/refinery/current/static_data/mediawiki/grouped_wikis/grouped_wikis.csv" \
#     --timestamp YYYYMMDD000000 \
#     --user research \
#     --password-file /user/hdfs/mysql-analytics-research-client-pw.txt

"""
Sqoops a list of tables from a list of wikis into a target HDFS location

Usage:
  sqoop-mediawiki-tables --jdbc-host HOST --output-dir HDFS_PATH
          [--verbose] --wiki-file WIKIS --timestamp TIME
          [--max-tries TRIES] [--force] --snapshot SNAPSHOT
          [--mappers NUM] [--processors NUM] --user NAME
          [--job-name JOB_NAME] [--labsdb] --password-file FILE
          [--generate-jar JAR_OUT|--jar-file JAR_IN]

Options:
    -h --help                           Show this help message and exit.
    -v --verbose                        Turn on verbose debug logging.

    -H HOST --jdbc-host HOST            Domain name of the mysql db
    -d HDFS_PATH --output-dir HDFS_PATH Target hdfs directory to write to

    -w FILE --wiki-file FILE            File with list of wiki dbs to sqoop
                                          A csv file of the form:

                                          dbname,parallel-group,...

                                          where all wiki dbs that will be
                                          sqooped in parallel with this one
                                          share the same parallel-group
    -t TIME --timestamp TIME            Try to get revisions only before this
                                        (not exact due to new data coming in with
                                        old timestamps; also, this only applies to
                                        the revision table, not other tables)
    -s SNAPSHOT --snapshot SNAPSHOT     The snapshot partition to load data
                                        into (usually YYYY-MM)
    -u NAME --user NAME                 mysql user to use
    -p FILE --password-file FILE        File with mysql password to use

    -m NUM --mappers NUM                The number of mappers to use to sqoop big tables
                                        If the group of wikis from --wiki-file is larger
                                        than 3 wikis, this number will get changed to 1
                                        regardless of what is passed in, to prevent too
                                        many parallel connections to the source database
                                        [optional] default is 1
    -k NUM --processors NUM             The number of parallel processors sqooping
                                        If the group of wikis from --wiki-file is smaller
                                        than 3 wikis, this number will be changed to 1
                                        regardless of what is passed in, to prevent too
                                        many parallel sqoop jobs on a big wiki
                                        [optional] default is the number of
                                        processors on the machine
    -j JOB_NAME --job-name JOB_NAME     The yarn job name prefix, only one job with
                                        a certain prefix can run at a time.
                                        [optional] default is sqoop-mediawiki-tables
    -r TRIES --max-tries TRIES          Maximum number of tries for a sqoop job in
                                        case of failure [default: 3]
    -l --labsdb                         Add '_p' postfix to table names for labsdb
    -f --force                          Deleting existing folders before importing
                                        instead of failing
    -g JAR_OUT --generate-jar JAR_OUT   Instead of running the job, just pick one wiki
                                        and generate the java classes for each table,
                                        then bundle them into a JAR for later use. Save
                                        the generated jar to JAR_OUT, in a file named
                                        JAR_OUT/mediawiki-tables-sqoop-orm.jar
                                        NOTE: etwiki will be used to generate ORM classes
    -r JAR_IN --jar-file JAR_IN         Disable code generation and use a jar file
                                        with pre-compiled ORM classes.  The class names
                                        will be convention-based and assumed to be the
                                        same as running this script with -g
"""
__author__ = 'Dan Andreesu <milimetric@wikimedia.org>'

import csv
import logging
import os
import sys

from docopt import docopt
from concurrent import futures
from itertools import groupby
from subprocess import check_call
from traceback import format_exc
from tempfile import mkstemp

from refinery.util import is_yarn_application_running, HdfsUtils

queries = {}

print('************ NOTE ************')
print('When sqooping from labs, resulting data will be shareable with the public '
      'but when sqooping from production, resulting data may need to be redacted or '
      'otherwise anonymized before sharing.')
print('^^^^^^^^^^^^ NOTE ^^^^^^^^^^^^')


def populate_queries(timestamp, labsdb):

    # NOTES on queries:
    # convert(... using utf8) is used to decode varbinary fields into strings
    # type mapping is used to handle some databases having booleans in
    #   tinyint(1) and others in tinyint(3,4) (newer databases like wikivoyage)

    queries['archive'] = {
        'query': '''
             select ar_id,
                    ar_namespace,
                    convert(ar_title using utf8) ar_title,
                    convert(ar_text using utf8) ar_text,
                    convert(ar_comment using utf8) ar_comment,
                    ar_user,
                    convert(ar_user_text using utf8) ar_user_text,
                    convert(ar_timestamp using utf8) ar_timestamp,
                    ar_minor_edit,
                    convert(ar_flags using utf8) ar_flags,
                    ar_rev_id,
                    ar_text_id,
                    ar_deleted,
                    ar_len,
                    ar_page_id,
                    ar_parent_id,
                    convert(ar_sha1 using utf8) ar_sha1,
                    convert({model} using utf8) ar_content_model,
                    convert({format} using utf8) ar_content_format

               from archive
              where $CONDITIONS
        '''.format(model="''" if labsdb else 'ar_content_model',
                   format="''" if labsdb else 'ar_content_format'),
        'map-types': '"{}"'.format(','.join([
            'ar_minor_edit=Boolean',
            'ar_deleted=Integer',
        ])),

        'split-by': 'ar_id',
    }

    queries['ipblocks'] = {
        'query': '''
             select ipb_id,
                    convert(ipb_address using utf8) ipb_address,
                    ipb_user,
                    ipb_by,
                    convert(ipb_by_text using utf8) ipb_by_text,
                    convert(ipb_reason using utf8) ipb_reason,
                    convert(ipb_timestamp using utf8) ipb_timestamp,
                    ipb_auto,
                    ipb_anon_only,
                    ipb_create_account,
                    ipb_enable_autoblock,
                    convert(ipb_expiry using utf8) ipb_expiry,
                    convert(ipb_range_start using utf8) ipb_range_start,
                    convert(ipb_range_end using utf8) ipb_range_end,
                    ipb_deleted,
                    ipb_block_email,
                    ipb_allow_usertalk,
                    ipb_parent_block_id

               from ipblocks
              where $CONDITIONS
        ''',
        'map-types': '"{}"'.format(','.join([
            'ipb_auto=Boolean',
            'ipb_anon_only=Boolean',
            'ipb_create_account=Boolean',
            'ipb_enable_autoblock=Boolean',
            'ipb_deleted=Boolean',
            'ipb_block_email=Boolean',
            'ipb_allow_usertalk=Boolean',
        ])),

        'split-by': 'ipb_id',
    }

    queries['logging'] = {
        'query': '''
             select log_id,
                    convert(log_type using utf8) log_type,
                    convert(log_action using utf8) log_action,
                    convert(log_timestamp using utf8) log_timestamp,
                    log_user,
                    log_namespace,
                    convert(log_title using utf8) log_title,
                    convert(log_comment using utf8) log_comment,
                    convert(log_params using utf8) log_params,
                    log_deleted,
                    convert(log_user_text using utf8) log_user_text,
                    log_page

               from logging
              where $CONDITIONS
        ''',

        'split-by': 'log_id',
    }

    queries['page'] = {
        'query': '''
             select page_id,
                    page_namespace,
                    convert(page_title using utf8) page_title,
                    convert(page_restrictions using utf8) page_restrictions,
                    page_is_redirect,
                    page_is_new,
                    page_random,
                    convert(page_touched using utf8) page_touched,
                    convert(page_links_updated using utf8) page_links_updated,
                    page_latest,
                    page_len,
                    convert(page_content_model using utf8) page_content_model

               from page
              where $CONDITIONS
        ''',
        'map-types': '"{}"'.format(','.join([
            'page_is_redirect=Boolean',
            'page_is_new=Boolean',
        ])),

        'split-by': 'page_id',
    }

    queries['pagelinks'] = {
        'query': '''
             select pl_from,
                    pl_namespace,
                    convert(pl_title using utf8) pl_title,
                    pl_from_namespace

               from pagelinks
              where $CONDITIONS
        ''',

        'split-by': 'pl_from',
    }

    queries['redirect'] = {
        'query': '''
             select rd_from,
                    rd_namespace,
                    convert(rd_title using utf8) rd_title,
                    convert(rd_interwiki using utf8) rd_interwiki,
                    convert(rd_fragment using utf8) rd_fragment

               from redirect
              where $CONDITIONS
        ''',

        'split-by': 'rd_from',
    }

    queries['revision'] = {
        'query': '''
             select rev_id,
                    rev_page,
                    rev_text_id,
                    convert(rev_comment using utf8) rev_comment,
                    rev_user,
                    convert(rev_user_text using utf8) rev_user_text,
                    convert(rev_timestamp using utf8) rev_timestamp,
                    rev_minor_edit,
                    rev_deleted,
                    rev_len,
                    rev_parent_id,
                    convert(rev_sha1 using utf8) rev_sha1,
                    convert(rev_content_model using utf8) rev_content_model,
                    convert(rev_content_format using utf8) rev_content_format

               from revision
              where $CONDITIONS
                and rev_timestamp <= '{t}'
        '''.format(t=timestamp),
        'map-types': '"{}"'.format(','.join([
            'rev_minor_edit=Boolean',
            'rev_deleted=Integer',
        ])),

        'split-by': 'rev_id',
    }

    queries['user'] = {
        'query': '''
             select user_id,
                    convert(user_name using utf8) user_name,
                    user_name user_name_binary,
                    convert(user_real_name using utf8) user_real_name,
                    convert(user_email using utf8) user_email,
                    convert(user_touched using utf8) user_touched,
                    convert(user_registration using utf8) user_registration,
                    user_editcount,
                    convert(user_password_expires using utf8) user_password_expires

               from user
              where $CONDITIONS
        ''',

        'split-by': 'user_id',
    }

    queries['user_groups'] = {
        'query': '''
             select ug_user,
                    convert(ug_group using utf8) ug_group

               from user_groups
              where $CONDITIONS
        ''',

        'split-by': 'ug_user',
    }


class SqoopConfig:

    def __init__(self, yarn_job_name_prefix, user, password_file, jdbc_host, num_mappers,
                 table_path_template, dbname, dbpostfix, table,
                 query, split_by, map_types,
                 generate_jar, jar_file,
                 current_try):

        self.yarn_job_name_prefix = yarn_job_name_prefix
        self.user = user
        self.password_file = password_file
        self.jdbc_host = jdbc_host
        self.num_mappers = num_mappers
        self.table_path_template = table_path_template
        self.dbname = dbname
        self.dbpostfix = dbpostfix
        self.table = table
        self.query = query
        self.split_by = split_by
        self.map_types = map_types
        self.generate_jar = generate_jar
        self.jar_file = jar_file
        self.current_try = current_try


def sqoop_wiki(config):
    """
    Imports a pre-determined list of tables from dbname

    Parameters
        config: SqoopConfig object filed in with needed parameters

    Returns
        True if the sqoop worked
        False if the sqoop errored or failed in any way
    """
    full_table = '.'.join([config.dbname, config.table])
    log_message = '{} (try {})'.format(full_table, config.current_try)
    logging.info('STARTING: {}'.format(log_message))
    try:
        target_directory = (config.table_path_template + '/wiki_db={db}').format(
            table=config.table, db=config.dbname)

        query = config.query
        command = 'import'
        if config.generate_jar:
            query = query + ' and 1=0'
            command = 'codegen'

        sqoop_arguments = [
            'sqoop',
            command,
            '-D'                , "mapred.job.name='{}-{}'".format(
                config.yarn_job_name_prefix, full_table),
            '--username'        , config.user,
            '--password-file'   , config.password_file,
            '--connect'         , config.jdbc_host + config.dbname + config.dbpostfix,
            '--query'           , config.query,
        ]

        if config.generate_jar:
            sqoop_arguments += [
                '--class-name'      , config.table,
                '--outdir'          , config.generate_jar,
                '--bindir'          , config.generate_jar,
            ]
        else:
            sqoop_arguments += [
                '--target-dir'      , target_directory,
                '--num-mappers'     , str(config.num_mappers),
                '--as-avrodatafile' ,
            ]
            if config.num_mappers > 1:
                sqoop_arguments += [
                    '--split-by'    , config.split_by,
                ]

        if config.jar_file:
            sqoop_arguments += [
                '--class-name'      , config.table,
                '--jar-file'        , config.jar_file,
            ]

        if config.map_types:
            sqoop_arguments += [
                '--map-column-java' , config.map_types
            ]

        # Force deletion of possibly existing dir in case of retry
        if config.current_try > 1:
            sqoop_arguments += [
                '--delete-target-dir'
            ]

        logging.info('Sqooping with: {}'.format(sqoop_arguments))
        check_call(sqoop_arguments)
        logging.info('FINISHED: {}'.format(log_message))
        return None
    except:
        logging.error('ERROR: {}, {}'.format(log_message, format_exc()))
        config.current_try += 1
        return config


def check_hdfs_path(table_path_template, force):
    res = True
    logging.info('Checking HDFS paths')
    for table in queries.keys():
        table_path = table_path_template.format(table=table)

        if HdfsUtils.ls(table_path, include_children=False):
            if force:
                HdfsUtils.rm(table_path)
                logging.info('Forcing: {} deleted from HDFS.'.format(table_path))
            else:
                logging.error('{} already exists in HDFS.'.format(table_path))
                res = False
    return res


if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)
    verbose                             = arguments.get('--verbose')
    labsdb                              = arguments.get('--labsdb')
    yarn_job_name_prefix                = arguments.get('--job-name')

    host                                = arguments.get('--jdbc-host')
    target_hdfs_directory               = arguments.get('--output-dir')
    db_list_file                        = arguments.get('--wiki-file')
    timestamp                           = arguments.get('--timestamp')
    snapshot                            = arguments.get('--snapshot')
    user                                = arguments.get('--user')
    password_file                       = arguments.get('--password-file')
    num_mappers                         = int(arguments.get('--mappers') or '1')
    num_processors                      = int(arguments.get('--processors')) if arguments.get('--processors') else None
    max_tries                           = int(arguments.get('--max-tries'))
    force                               = arguments.get('--force')
    generate_jar                        = arguments.get('--generate-jar')
    jar_file                            = arguments.get('--jar-file')

    table_path_template = '{hdfs}/{table}/snapshot={snapshot}'.format(
        hdfs=target_hdfs_directory,
        table='{table}',
        snapshot=snapshot,
    )

    log_level = logging.INFO
    if verbose:
        log_level = logging.DEBUG

    logging.basicConfig(level=log_level,
                        format='%(asctime)s %(levelname)-6s %(message)s',
                        datefmt='%Y-%m-%dT%H:%M:%S')

    yarn_job_name_prefix = yarn_job_name_prefix or 'sqoop-mediawiki-tables'
    # This works since the check doesn't involve 'full word' matching
    if is_yarn_application_running(yarn_job_name_prefix):
        logging.warn('{} is already running, exiting.'.format(yarn_job_name_prefix))
        sys.exit(1)

    jdbc_host = 'jdbc:mysql://' + host + '/'

    logging.info('Started Shell with with {}'.format(' '.join(arguments)))

    populate_queries(timestamp, labsdb)
    dbpostfix = '_p' if labsdb else ''

    if not check_hdfs_path(table_path_template, force):
        sys.exit(1)

    if generate_jar:
        flat_wikis = [['etwiki', 1]]
        jar_path = os.path.join(generate_jar, 'mediawiki-tables-sqoop-orm.jar')
    else:
        # read in the wikis to process and sqoop each one
        with open(db_list_file) as dbs_file:
            # Remove lines starting with dashes
            flat_wikis = [row for row in csv.reader(dbs_file) if not row[0].startswith('#')]

    failed_jobs = []
    for group, wikis in groupby(flat_wikis, lambda w: w[1]):
        executor_config_list = []

        # If there are many wikis in a group set num_mappers to 1
        # This is because you don't want to sqoop in parallel when the amount of data is small
        # Also, do the opposite with num_processors for the same reason
        num_mappers_adjusted = num_mappers
        num_processors_adjusted = 1
        if len(wikis) > 3:
            num_mappers_adjusted = 1
            num_processors_adjusted = num_processors

        for w in wikis:
            for table in queries.keys():
                query = queries[table].get('query')
                map_types = queries[table]['map-types'] if ('map-types' in queries[table]) else None
                split_by = queries[table]['split-by']
                executor_config_list.append(SqoopConfig(yarn_job_name_prefix, user, password_file, jdbc_host,
                                            num_mappers_adjusted,
                                            table_path_template, w[0], dbpostfix, table,
                                            query, split_by, map_types,
                                            generate_jar, jar_file, 1))

        # sqoop all wikis in this group and wait for them all to finish with retry
        with futures.ProcessPoolExecutor(num_processors_adjusted) as executor:
            current_try = 0
            while (executor_config_list and current_try < max_tries):
                executor_config_list = filter(None, list(executor.map(sqoop_wiki, executor_config_list)))
                current_try += 1
        failed_jobs.extend(executor_config_list)

    # if there were no failures at all, write a success flag to this dataset
    if not failed_jobs:
        if not generate_jar:
            for table in queries.keys():
                success_directory = table_path_template.format(table=table)
                check_call([
                    'hdfs', 'dfs', '-touchz',
                    success_directory + '/_SUCCESS',
                ])
        else:
            check_call([
                'jar', 'cf', jar_path, '-C', generate_jar, '.'
            ])
            logging.info('Generated ORM jar at {}'.format(jar_path))
    else:
        to_rerun = ','.join(failed_jobs)
        logging.error('*' * 50)
        logging.error('*  Jobs to re-run: {}'.format(to_rerun))
        for c in failed_jobs:
            logging.error('*    - {}.{}'.format(c.dbname, c.table))
        logging.error('*' * 50)
        sys.exit(1)
