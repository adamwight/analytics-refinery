#!/bin/bash

set -e

print_help() {
    cat <<EOF
$0 [ OPTIONS ] [ HOURS_TO_GO_BACK ]

dumps the status of the raw webrequest partitions for the last few hours.

Options:
  --hdfs-mount MOUNT_POINT
                  -- Assume that HDFS is mounted at MOUNT_POINT (needs
                     to be an absolute path) instead of /mnt/hdfs .
  --datasets DATASET1,DATASET2,...
                  -- Select the datasets to output data for.
                     The following datasets are available:
                       pagecounts-all-sites -- pagecounts-all-sites (hourly)
                       raw_webrequest       -- Raw webrequest (hourly)
                       webrequest           -- webrequest (refined tables) (hourly)
                       all                  -- all of the above

                     By default, only "raw_webrequest" is shown.

  --quiet         -- Only produce output, if there are faulty partitions

HOURS_TO_GO_BACK  -- number of hours to go back in time. (Default: 51)

EOF
}


HOUR_OFFSET_MAX=51
HOUR_OFFSET_MIN=3

HDFS_MOUNT_DIR_ABS=/mnt/hdfs

HAS_FAULTY=
QUIET=
QUIET_BUFFER=

ALL_DATASETS=()

declare -A DATASET_CAPTIONS
declare -A DATASET_HLINES
declare -A DATASET_VISIBILITIES

add_dataset() {
    local DATASET="$1"
    local DATASET_CAPTION="$2"
    local DATASET_HLINE="$(sed -e 's/[^|]/-/g; s/|/+/g' <<<"$DATASET_CAPTION")"

    ALL_DATASETS=( "${ALL_DATASETS[@]}" "$DATASET" )

    DATASET_CAPTIONS["$DATASET"]="$DATASET_CAPTION"
    DATASET_HLINES["$DATASET"]="$DATASET_HLINE"
    DATASET_VISIBILITIES["$DATASET"]=no
}

add_dataset "pagecounts_all_sites" " file name date  |  page   | project |"
add_dataset "raw_webrequest" "  bits  |  misc  | mobile |  text  | upload |"
add_dataset "webrequest" "  bits  |  misc  | mobile |  text  | upload |"

DATASET_VISIBILITIES["raw_webrequest"]=yes

error() {
    echo "Error" "$@" >&2
    exit 1
}

while [ $# -gt 0 ]
do
    PARAM="$1"
    shift
    case "$PARAM" in
        "--help" | "-h" | "-?" )
            print_help
            exit 1
            ;;
        "--datasets" )
            [[ $# -gt 0 ]] || error "$PARAM expects a further parameter"

            # Resetting previous visibilities
            for INNER_DATASET in "${ALL_DATASETS[@]}"
            do
                DATASET_VISIBILITIES["$INNER_DATASET"]=no
            done

            IFS="," read -a DATASETS_SPLIT <<<"$1"
            for DATASET in "${DATASETS_SPLIT[@]}"
            do
                case "$DATASET" in
                    "all" )
                        for INNER_DATASET in "${ALL_DATASETS[@]}"
                        do
                            DATASET_VISIBILITIES["$INNER_DATASET"]=yes
                        done
                        ;;
                    * )
                        FOUND_DATASET=no
                        for INNER_DATASET in "${ALL_DATASETS[@]}"
                        do
                            if [ "${DATASET//-/_}" = "$INNER_DATASET" ]
                            then
                                DATASET_VISIBILITIES["$INNER_DATASET"]=yes
                                FOUND_DATASET=yes
                            fi
                        done
                        if [ "$FOUND_DATASET" != "yes" ]
                        then
                            error "Unknown dataset '$DATASET '"
                        fi
                        ;;
                esac
            done
            shift
            ;;
        "--hdfs-mount" )
            [[ $# -gt 0 ]] || error "$PARAM expects a further parameter"
            HDFS_MOUNT_DIR_ABS="$1"
            shift
            ;;
        "--quiet" )
            QUIET=yes
            ;;
        * )
            if [ $# -eq 0 ]
            then
                HOUR_OFFSET_MAX="$PARAM"
            else
                error "Too many parameters given"
            fi
            ;;
    esac
done

RAW_WEBREQUEST_DATA_DIR_ABS="$HDFS_MOUNT_DIR_ABS/wmf/data/raw/webrequest"
RAW_WEBREQUEST_STATISTICS_DIR_ABS="$HDFS_MOUNT_DIR_ABS/wmf/data/raw/webrequests_faulty_hosts"
WEBREQUEST_DATA_DIR_ABS="$HDFS_MOUNT_DIR_ABS/wmf/data/wmf/webrequest"
ARCHIVE_DATA_DIR_ABS="$HDFS_MOUNT_DIR_ABS/wmf/data/archive"

log_no_lf() {
    if [ -n "$QUIET" ]
    then
        QUIET_BUFFER="$QUIET_BUFFER$@"
        if [ -n "$HAS_FAULTY" ]
        then
            echo -n "$QUIET_BUFFER"
            QUIET_BUFFER=
        fi
    else
        echo -n "$@"
    fi
}

log_no_lf_centered() {
    local TEXT="$1"
    local AVAILABLE_LEN="$2"

    local BLANK_HELPER="                                                       "

    local TEXT_LEN="${#TEXT}"

    log_no_lf "${BLANK_HELPER:0:$(( (AVAILABLE_LEN-TEXT_LEN) / 2 ))}"
    log_no_lf "$TEXT"
    log_no_lf "${BLANK_HELPER:0:$(( AVAILABLE_LEN - (AVAILABLE_LEN-TEXT_LEN) / 2 - TEXT_LEN ))}"
}

log() {
    log_no_lf "$@
"
}

hline() {
    local KIND="$1"

    log_no_lf "  +------------------+"
    for DATASET in "${ALL_DATASETS[@]}"
    do
        if [ "${DATASET_VISIBILITIES["$DATASET"]}" = "yes" ]
        then
            local DATASET_HLINE="${DATASET_HLINES["$DATASET"]}"
            if [ "$KIND" = "first" ]
            then
                DATASET_HLINE="${DATASET_HLINE//+-/--}"
            fi
            log_no_lf "${DATASET_HLINE}"
        fi
    done
    log
}

first_caption_line() {
    local DATASET

    log_no_lf "  |                  |"
    for DATASET in "${ALL_DATASETS[@]}"
    do
        if [ "${DATASET_VISIBILITIES["$DATASET"]}" = "yes" ]
        then
            local DATASET_CAPTION="${DATASET_CAPTIONS["$DATASET"]}"
            local DATASET_CAPTION_LEN="${#DATASET_CAPTION}"
            log_no_lf_centered "$DATASET" $((DATASET_CAPTION_LEN-1))
            log_no_lf "|"
        fi
    done
    log
}

second_caption_line() {
    local DATASET

    log_no_lf "  | Hour             |"
    for DATASET in "${ALL_DATASETS[@]}"
    do
        if [ "${DATASET_VISIBILITIES["$DATASET"]}" = "yes" ]
        then
            log_no_lf "${DATASET_CAPTIONS["$DATASET"]}"
        fi
    done
    log
}

dump_dataset_pagecounts_file() {
    local DATASET="$1"
    local KIND="$2"

    local STATUS="X"

    if [ "$KIND" = page ]
    then
        FILE_ENDING=".gz"
    else
        FILE_ENDING=""
    fi

    FILE_DATE_PART="$(date --utc -d "$DATE 1 hour" +"%Y/%Y-%m/${KIND}counts-%Y%m%d-%H0000")"

    FILE_ABS="$ARCHIVE_DATA_DIR_ABS/${DATASET//_/-}/$FILE_DATE_PART$FILE_ENDING"

    if [ -e "$FILE_ABS" ]
    then
        STATUS="."
    fi
    log_no_lf "$STATUS"
}

dump_dataset_pagecounts_all_sites() {
    local DATE="$1"
    local DATASET="pagecounts_all_sites"

    log_no_lf " $(date --utc -d "$DATE 1 hour" +'%Y%m%d-%H0000') |"
    for KIND in page project
    do
        log_no_lf "    "
        dump_dataset_pagecounts_file "$DATASET" "$KIND"
        log_no_lf "    |"
    done
}

dump_dataset_raw_webrequest_partition() {

    local DATE_HDFS_PADDED="$1"
    local SOURCE="$2"
    local STATUS="X"

    local DATE_HDFS_UNPADDED="${DATE_HDFS_PADDED///0//}"

    STATISTICS_FILE_ABS="$RAW_WEBREQUEST_STATISTICS_DIR_ABS/$SOURCE/$DATE_HDFS_UNPADDED/000000_0"
    if [ -e "$STATISTICS_FILE_ABS" -a ! -s "$STATISTICS_FILE_ABS" ]
    then
        STATUS="."
    else
        if [ -e "$RAW_WEBREQUEST_DATA_DIR_ABS/webrequest_$SOURCE/hourly/$DATE_HDFS_PADDED/_SUCCESS" ]
        then
            STATUS="M"
        else
            STATUS="X"
            HAS_FAULTY=yes
        fi
    fi
    log_no_lf "$STATUS"
}

dump_dataset_raw_webrequest() {
    local DATE="$1"

    local DATE_HDFS_PADDED="$(date --utc -d "$DATE" +'%Y/%m/%d/%H')"

    for SOURCE in bits misc mobile text upload
    do
        log_no_lf "    "
        dump_dataset_raw_webrequest_partition "$DATE_HDFS_PADDED" "$SOURCE"
        log_no_lf "   |"
    done
}

dump_dataset_webrequest() {
    local DATE="$1"

    local DATE_DIRS_REL="$(date --utc -d "$DATE" +'year=%Y/month=%m/day=%d/hour=%H')"
    DATE_DIRS_REL="${DATE_DIRS_REL//=0/=}"

    for SOURCE in bits misc mobile text upload
    do
        local STATUS="X"
        SUCCESS_FILE_ABS="$WEBREQUEST_DATA_DIR_ABS/webrequest_source=$SOURCE/$DATE_DIRS_REL/_SUCCESS"
        if [ -e "$SUCCESS_FILE_ABS" ]
        then
            STATUS="."
        fi
        log_no_lf "    $STATUS   |"
    done
}

hline "first"
first_caption_line
second_caption_line
hline

for HOURS_OFFSET in $(seq $HOUR_OFFSET_MAX -1 $HOUR_OFFSET_MIN )
do
    DATE="$(date --utc -d "$HOURS_OFFSET hours-ago" +'%Y-%m-%d %H')"
    log_no_lf "  | ${DATE// /T}/1H |"
    for DATASET in "${ALL_DATASETS[@]}"
    do
        if [ "${DATASET_VISIBILITIES["$DATASET"]}" = "yes" ]
        then
            dump_dataset_$DATASET "$DATE"
        fi
    done
    log
done

hline

log "

Statuses:

  . --> Partition is ok
  M --> Partition manually marked ok
  X --> Partition is not ok (duplicates, missing, or nulls)


"
