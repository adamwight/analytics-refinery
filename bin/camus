#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""" Wrapper for launching Camus Kafka Importer and CamusPartitionChecker

Usage: camus [options] <properties-file>

Options:
    -h --help                           Show this help message and exit.
    -n --dry-run                        Prints command that would have been run without running anything.
    -r --run                            If set, a camus job will be launched using the given configuration
    -N --job-name=<job-name>            Hadoop JobName. [default: Camus Job]
    -j --jar=<jar-file>                 Path to Camus .jar file.  [default: /srv/deployment/analytics/refinery/artifacts/camus-wmf.jar]
    -l --libjars=<jar-file(s)>          Comma separated paths to external jars to make available to Camus' Map Reduce jobs
    -f --force                          If set, job will be submitted without checking if it is already running.
    -c --check                          If set, a CamusPartitionChecker job will be submitted after the camus run if any (checking and flagging imported partitions).
    -q --check-jar=<jar-file>           Path to refinery-job jar file. [default: /srv/deployment/analytics/refinery/artifacts/refinery-job.jar]
    -d --check-datetime=<datetime>      Camus run date to check (yyyy-mm-dd-HH-MM-SS format). Most recent run if not set.
    -m --check-dry-run                  Execute the check in dry-run mode.
"""
__author__ = 'Andrew Otto <otto@wikimedia.org>'

import os
import sys
import logging
from docopt import docopt



logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)-6s %(message)s',
                    datefmt='%Y-%m-%dT%H:%M:%S')

def is_yarn_application_running(job_name):
    '''
    Returns true if job_name is found in the output
    of yarn application -list and has a status of
    RUNNING or ACCEPTED.  Returns false otherwise.

    Note:  Error checking on the yarn shell command is not good.
    This command will return false on any command failure.
    '''
    command = '/usr/bin/yarn application -list 2>/dev/null | grep -q  "\({0}\).*\(RUNNING\|ACCEPTED\)"'.format(job_name)
    logging.debug('Running: {0}'.format(command))

    retval = os.system(command)
    return retval == 0

if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)
    exit_value = 0

    properties_file = arguments['<properties-file>']
    dry_run = arguments['--dry-run']

    #Camus execution block
    if (arguments['--run']):
        jar             = arguments['--jar']
        job_name        = arguments['--job-name']
        libjars         = arguments['--libjars']

        if not arguments['--force'] and is_yarn_application_running(job_name):
            logging.warn('Not submitting camus job "{0}", it is currently running.'.format(job_name))
            sys.exit(1)

        libjars_opt = '' if not libjars else ('-libjars ' + libjars)
        camus_command = '/usr/bin/hadoop jar {0} com.linkedin.camus.etl.kafka.CamusJob {1} -P {2} -Dcamus.job.name="{3}"'.format(
            jar,
            libjars_opt,
            properties_file,
            job_name
        )
        logging.info('Submitting camus job "{0}": {1}'.format(job_name, camus_command))

        camus_res = 0
        if not dry_run:
            exit_value += os.system(camus_command)

    # Check execution block
    if (arguments['--check']):
        check_jar       = arguments['--check-jar']
        # Spark classpath contains hadoop jars, no need to add them explicitely
        check_classpath = ':'.join([check_jar, '/usr/lib/spark/lib/spark-assembly.jar', '$(hadoop classpath | sed \'s@^[^:]*:@@\')', '/usr/share/java/*'])
        datetime_to_check = '-d %s' % arguments['--check-datetime'] if arguments['--check-datetime'] else ''
        checker_command = '/usr/bin/java -cp "{0}" org.wikimedia.analytics.refinery.job.CamusPartitionChecker -c {1} {2}'.format(
            check_classpath,
            properties_file,
            datetime_to_check
        )

        logging.info('Submitting camus-checker job: {0}'.format(checker_command))
        if not dry_run:
            exit_value += os.system(checker_command + (' --dry-run' if arguments['--check-dry-run'] else ''))

    sys.exit(exit_value)
