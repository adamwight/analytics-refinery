#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Note: You should make sure to put refinery/python on your PYTHONPATH.
#   export PYTHONPATH=$PYTHONPATH:/path/to/refinery/python

"""
Automatically drops old Hive partitions from Hive tables
and deletes the hourly time bucketed directories from HDFS.

NOTE: This script should replace refinery-drop-hourly-partitions once
all uses are ported over to it.

Usage: refinery-drop-hive-partitions [options]

Options:
    -h --help                           Show this help message and exit.
    -d --older-than-days=<days>         Drop data older than this number of days.  [default: 60]
    -D --database=<dbname>              Hive database name.  [default: default]
    -t --tables=<tables>                Comma separated list of tables to check.  Defaults to
                                        all tables in database.
    -l --limit=<limit>                  Only drop from this many tables.  Useful for testing.
    -N --partition-depth=<n>            Number of directories to use when constructing partition file glob.
                                        If not given, this will be infered from the first partition
                                        found in the table.  If the table has noÂ current partitions,
                                        this will throw an exeption.
    -o --hive-options=<options>         Any valid Hive CLI options you want to pass to Hive commands.
                                        Example: '--auxpath /path/to/hive-serdes-1.0-SNAPSHOT.jar'
    -v --verbose                        Turn on verbose debug logging.
    -n --dry-run                        Don't actually drop any partitions, just output the Hive queries to drop partitions.
"""
__author__ = 'Andrew Otto <otto@wikimedia.org>'

import datetime
from   docopt   import docopt
import logging
import re
import os
import sys
from refinery.util import HiveUtils, HivePartition, HdfsUtils


if __name__ == '__main__':
    # parse arguments
    arguments = docopt(__doc__)

    days            = int(arguments['--older-than-days'])
    database        = arguments['--database']
    tables          = arguments['--tables']
    limit           = arguments['--limit']
    partition_depth = arguments['--partition-depth']
    hive_options    = arguments['--hive-options']
    verbose         = arguments['--verbose']
    dry_run         = arguments['--dry-run']


    log_level = logging.INFO
    if verbose:
        log_level = logging.DEBUG

    logging.basicConfig(level=log_level,
                        format='%(asctime)s %(levelname)-6s %(message)s',
                        datefmt='%Y-%m-%dT%H:%M:%S')

    if partition_depth is not None:
        partition_depth = int(partition_depth)

    # Delete partitions older than this.
    old_partition_datetime_threshold = datetime.datetime.now() - datetime.timedelta(days=days)

    # Instantiate HiveUtils.
    hive = HiveUtils(database, hive_options)

    if tables is not None:
        tables = tables.split(',')
    else:
        tables = hive.get_tables()

    if limit is not None:
        tables = tables[:int(limit)]

    # Iterate through each table and find old Hive partitions and HDFS paths to drop.
    for table in tables:
        logging.info('Looking for partitions to drop for {}.{}...'.format(database, table))
        # Attempt to infer table location from the table metadata.
        table_location = hive.table_location(table)

        partitions_to_drop        = []
        partition_paths_to_delete = []

        # Loop through all partitions for this table and drop anything that is too old.
        hive_partitions = hive.partitions(table)
        for partition in hive_partitions:
            if partition.datetime() < old_partition_datetime_threshold:
                partitions_to_drop.append(partition)

        # Managed tables have their files deleted by hive directly from the
        # drop table statement. Other tables need manual cleanup.
        table_type = hive.table_metadata(table)['Table Type']
        if table_type != 'MANAGED_TABLE':
            # Build a glob based on number of partition keys found in Hive partitions.
            # This will be used to find possible directories that need to be removed
            # from HDFS. TODO: This has a bug in that no data will be removed from HDFS
            # If not partitions were in the Hive table.
            if partition_depth is not None:
                partition_glob = os.path.join(*([table_location] + ['*'] * partition_depth))
            elif len(hive_partitions) > 0:
                partition_glob = hive_partitions[0].glob(base_path=table_location)
            else:
                partition_glob = None
                logging.warn(
                    'Could not search for HDFS paths to drop. '
                    'Could not construct partition glob from a partition depth.'
                )

            if partition_glob is not None:
                # Loop through all the partition directory paths for this table
                # and check if any of them are old enough for deletion.
                for partition_path in HdfsUtils.ls(partition_glob, include_children=False):
                    try:
                        partition = HivePartition(partition_path)
                        if partition.datetime() < old_partition_datetime_threshold:
                            partition_paths_to_delete.append(partition_path)
                    except Exception as e:
                        logging.error(
                            'Could not parse date from {}. Skipping. ({})'
                            .format(partition_path, e)
                        )
                        continue


        # Drop any old Hive partitions
        if partitions_to_drop:
            partition_specs_to_drop = [p.spec() for p in partitions_to_drop]
            if dry_run:
                print(hive.drop_partitions_ddl(table, partition_specs_to_drop))
            else:
                logging.info('Dropping {0} Hive partitions from table {1}.{2}'
                    .format(len(partition_specs_to_drop), database, table)
                )
                hive.drop_partitions(table, partition_specs_to_drop)
        else:
            logging.info('No Hive partitions need dropped for table {0}.{1}'.format(database, table))

        # Delete any old HDFS data
        if partition_paths_to_delete:
            if dry_run:
                print('hdfs dfs -rm -R ' + ' '.join(partition_paths_to_delete))
            else:
                logging.info('Removing {0} partition directories for table {1}.{2} from {3}.'
                    .format(len(partition_paths_to_delete), database, table, table_location)
                )
                HdfsUtils.rm(' '.join(partition_paths_to_delete))
        else:
            logging.info('No partition directories need removed for table {0}.{1}'.format(database, table))
