<?xml version="1.0" encoding="UTF-8"?>
<coordinator-app xmlns="uri:oozie:coordinator:0.4"
    name="wikidata-wdqs_extract-coord"
    frequency="${coord:hours(1)}"
    start="${start_time}"
    end="${stop_time}"
    timezone="Universal">

    <parameters>

        <!-- Required properties. -->
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>
        <property><name>queue_name</name></property>

        <property><name>workflow_file</name></property>

        <property><name>start_time</name></property>
        <property><name>stop_time</name></property>

        <property><name>webrequest_datasets_file</name></property>
        <property><name>webrequest_data_directory</name></property>

        <property><name>hive_site_xml</name></property>

        <property><name>webrequest_table</name></property>
        <property><name>wdqs_extract_table</name></property>

        <property><name>send_error_email_workflow_file</name></property>
    </parameters>

    <controls>
        <!--
        By having materialized jobs not timeout, we ease backfilling incidents
        after recoverable hiccups on the dataset producers.
        -->
        <timeout>-1</timeout>

        <!--
        pageview aggregation is not too heavy, but we limit
        concurrency for resource sharing.

        Also note, that back-filling is not limited by the
        coordinator's frequency, so back-filling works nicely
        even-though the concurrency is low.
        -->
        <concurrency>4</concurrency>


        <!--
        Since we expect only one incarnation per hourly dataset, the
        default throttle of 12 is way to high, and there is not need
        to keep that many materialized jobs around.

        By resorting to 2, we keep the hdfs checks on the datasets
        low, while still being able to easily feed the concurrency.
        -->
        <throttle>8</throttle>
    </controls>

    <datasets>
        <!--
        Include refined and aqs datasets files.
        $webrequest_datasets_file will be used as the input events
        $aqs_datasets_file will be used as the output events
        -->
        <include>${webrequest_datasets_file}</include>
    </datasets>

    <input-events>
        <data-in name="misc_refined_input" dataset="webrequest_misc">
            <instance>${coord:current(0)}</instance>
        </data-in>
    </input-events>

    <action>
        <workflow>
            <app-path>${workflow_file}</app-path>
            <configuration>

                <property>
                    <name>year</name>
                    <value>${coord:formatTime(coord:nominalTime(), "y")}</value>
                </property>
                <property>
                    <name>month</name>
                    <value>${coord:formatTime(coord:nominalTime(), "M")}</value>
                </property>
                <property>
                    <name>day</name>
                    <value>${coord:formatTime(coord:nominalTime(), "d")}</value>
                </property>
                <property>
                    <name>hour</name>
                    <value>${coord:formatTime(coord:nominalTime(), "H")}</value>
                </property>

            </configuration>
        </workflow>
    </action>
</coordinator-app>
