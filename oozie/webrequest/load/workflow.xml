<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns="uri:oozie:workflow:0.4"
    name="load_webrequest-${table}-${webrequest_source},${year},${month},${day},${hour}-wf">

    <parameters>

        <!-- Default values for inner oozie settings -->
        <property>
            <name>oozie_launcher_queue_name</name>
            <value>${queue_name}</value>
        </property>
        <property>
            <name>oozie_launcher_memory</name>
            <value>256</value>
        </property>

        <!-- Required properties -->
        <property><name>queue_name</name></property>
        <property><name>name_node</name></property>
        <property><name>job_tracker</name></property>

        <property>
            <name>add_partition_workflow_file</name>
            <description>Workflow definition for adding a partition</description>
        </property>
        <property>
            <name>hive_site_xml</name>
            <description>hive-site.xml file path in HDFS</description>
        </property>
        <property>
            <name>table</name>
            <description>Hive table to partition.</description>
        </property>
        <property>
            <name>webrequest_source</name>
            <description>The partition's webrequest_source</description>
        </property>
        <property>
            <name>year</name>
            <description>The partition's year</description>
        </property>
        <property>
            <name>month</name>
            <description>The partition's month</description>
        </property>
        <property>
            <name>day</name>
            <description>The partition's day</description>
        </property>
        <property>
            <name>hour</name>
            <description>The partition's hour</description>
        </property>
        <property>
            <name>location</name>
            <description>HDFS path(s) naming the input dataset.</description>
        </property>
        <property>
            <name>statistics_table</name>
            <description>
                Hive table to write partition statistics to.
            </description>
        </property>
        <property>
            <name>statistics_hourly_table</name>
            <description>
                Hive table to write hourly aggregate partition statistics to.
            </description>
        </property>
        <property>
            <name>data_loss_check_directory_base</name>
            <description>
                Base directory in HDFS where to put data loss information
            </description>
        </property>
        <property>
            <name>error_data_loss_threshold</name>
            <description>If data loss (percent) is above this value
                the job is failed and refined not launched.
            </description>
        </property>
        <property>
            <name>warning_data_loss_threshold</name>
            <description>If data loss (percent) is above this value
                and less than error_data_loss_threshold an email is sent
                but refine is still launched.
            </description>
        </property>
        <property>
            <name>mark_directory_done_workflow_file</name>
            <description>Workflow for marking a directory done</description>
        </property>
        <property>
            <name>send_error_email_workflow_file</name>
            <description>Workflow for sending an error email</description>
        </property>
    </parameters>

    <start to="add_partition"/>

    <action name="add_partition">
        <sub-workflow>
            <app-path>${add_partition_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>partition_spec</name>
                    <value>webrequest_source='${webrequest_source}',year=${year},month=${month},day=${day},hour=${hour}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="mark_add_partition_done"/>
        <error to="send_error_email"/>
    </action>

    <!--
    This adds an empty _PARTITIONED done-flag file into
    The directory for which we just added a Hive partition.
    The webrequest_*_raw_partitioned dataset uses this.
    -->
    <action name="mark_add_partition_done">
        <sub-workflow>
            <app-path>${mark_directory_done_workflow_file}</app-path>
            <configuration>
                <property>
                    <name>directory</name>
                    <value>${location}</value>
                </property>
                <property>
                    <name>done_file</name>
                    <value>_PARTITIONED</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="generate_sequence_statistics"/>
        <error to="send_error_email"/>
    </action>

    <action name="generate_sequence_statistics">
        <hive xmlns="uri:oozie:hive-action:0.3">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <job-xml>${hive_site_xml}</job-xml>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
                <property>
                    <name>mapreduce.job.queuename</name>
                    <value>${queue_name}</value>
                </property>
            </configuration>

            <script>generate_sequence_statistics.hql</script>

            <param>source_table=${table}</param>
            <param>destination_table=${statistics_table}</param>
            <param>year=${year}</param>
            <param>month=${month}</param>
            <param>day=${day}</param>
            <param>hour=${hour}</param>
            <param>webrequest_source=${webrequest_source}</param>
        </hive>
        <ok to="generate_sequence_statistics_hourly"/>
        <error to="send_error_email"/>
    </action>


    <!-- TODO: Use the result of the hourly query to mark a dataset as _SUCCEEDED -->
    <action name="generate_sequence_statistics_hourly">
        <hive xmlns="uri:oozie:hive-action:0.3">
            <job-tracker>${job_tracker}</job-tracker>
            <name-node>${name_node}</name-node>
            <job-xml>${hive_site_xml}</job-xml>
            <configuration>
                <!--make sure oozie:launcher runs in a low priority queue -->
                <property>
                    <name>oozie.launcher.mapred.job.queue.name</name>
                    <value>${oozie_launcher_queue_name}</value>
                </property>
                <property>
                    <name>oozie.launcher.mapreduce.map.memory.mb</name>
                    <value>${oozie_launcher_memory}</value>
                </property>
                <property>
                    <name>mapreduce.job.queuename</name>
                    <value>${queue_name}</value>
                </property>
            </configuration>

            <script>generate_sequence_statistics_hourly.hql</script>

            <param>source_table=${statistics_table}</param>
            <param>destination_table=${statistics_hourly_table}</param>
            <param>year=${year}</param>
            <param>month=${month}</param>
            <param>day=${day}</param>
            <param>hour=${hour}</param>
            <param>webrequest_source=${webrequest_source}</param>
        </hive>
        <ok to="check_sequence_statistics"/>
        <error to="send_error_email"/>
    </action>

    <!-- We put checking sequence statistics into a separate workflow,
        as that allows to build the full path for the "faulty hosts"
        directory (including source, year, ...) as element value, and
        thereby allows to evade building it in EL via a nested concat
        construct to use it in <case />, which would not be readable
        at all. -->
    <action name="check_sequence_statistics">
        <sub-workflow>
            <app-path>${replaceAll(wf:appPath(), "/[^/]*$", "")}/check_sequence_statistics_workflow.xml</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>error_data_loss_directory</name>
                    <value>${data_loss_check_directory_base}/${webrequest_source}/${year}/${month}/${day}/${hour}/ERROR</value>
                </property>
                <property>
                    <name>warning_data_loss_directory</name>
                    <value>${data_loss_check_directory_base}/${webrequest_source}/${year}/${month}/${day}/${hour}/WARNING</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="mark_dataset_done"/>
        <error to="kill"/>
    </action>

    <action name="mark_dataset_done">
        <sub-workflow>
            <app-path>${mark_directory_done_workflow_file}</app-path>
            <configuration>
                <property>
                    <name>directory</name>
                    <value>${location}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="end"/>
        <error to="send_error_email"/>
    </action>

    <action name="send_error_email">
        <sub-workflow>
            <app-path>${send_error_email_workflow_file}</app-path>
            <propagate-configuration/>
            <configuration>
                <property>
                    <name>parent_name</name>
                    <value>${wf:name()}</value>
                </property>
                <property>
                    <name>parent_failed_action</name>
                    <value>${wf:lastErrorNode()}</value>
                </property>
                <property>
                    <name>parent_error_code</name>
                    <value>${wf:errorCode(wf:lastErrorNode())}</value>
                </property>
                <property>
                    <name>parent_error_message</name>
                    <value>${wf:errorMessage(wf:lastErrorNode())}</value>
                </property>
            </configuration>
        </sub-workflow>
        <ok to="kill"/>
        <error to="kill"/>
    </action>

    <kill name="kill">
        <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
    </kill>
    <end name="end"/>
</workflow-app>
