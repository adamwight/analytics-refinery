# Configures a coordinator to generate daily tsvs from webrequests table.
#
# Usage:
#     oozie job -run \
#         -config oozie/webrequest/legacy_tsvs/bundle.properties
#
# NOTE:  The $oozie_directory must be synced to HDFS so that all relevant
#        .xml files exist there when this job is submitted.


name_node                              = hdfs://analytics-hadoop
job_tracker                            = resourcemanager.analytics.eqiad.wmnet:8032
queue_name                             = default

# Base path in HDFS to refinery.
# When submitting this job for production, you should
# override this to point directly at a deployed
# directory name, and not the 'symbolic' 'current' directory.
# E.g.  /wmf/refinery/2015-01-05T17.59.18Z--7bb7f07
refinery_directory                     = ${name_node}/wmf/refinery/current

# HDFS path to artifacts that will be used by this job.
# E.g. refinery-hive.jar should exist here.
artifacts_directory                    = ${refinery_directory}/artifacts

# Base path in HDFS to oozie files.
# Other files will be used relative to this path.
oozie_directory                        = ${refinery_directory}/oozie

# HDFS paths to the coordinators to run.
# All of them are essentially the same coordinator and differ only in the
# webrequest_sources they depend on. This allows to for example turn off upload
# and have the coordinators that depend on upload block, while the coordinators
# that do not depend on upload continue to run.
coordinator_bits_file                  = ${oozie_directory}/webrequest/legacy_tsvs/coordinator_bits.xml
coordinator_bits_misc_mobile_text_file = ${oozie_directory}/webrequest/legacy_tsvs/coordinator_bits_misc_mobile_text.xml
coordinator_misc_file                  = ${oozie_directory}/webrequest/legacy_tsvs/coordinator_misc.xml
coordinator_mobile_file                = ${oozie_directory}/webrequest/legacy_tsvs/coordinator_mobile.xml
coordinator_mobile_text_file           = ${oozie_directory}/webrequest/legacy_tsvs/coordinator_mobile_text.xml
coordinator_mobile_text_upload_file    = ${oozie_directory}/webrequest/legacy_tsvs/coordinator_mobile_text_upload.xml
coordinator_text_file                  = ${oozie_directory}/webrequest/legacy_tsvs/coordinator_text.xml
coordinator_upload_file                = ${oozie_directory}/webrequest/legacy_tsvs/coordinator_upload.xml

# HDFS path to workflow to run.
workflow_file                          = ${oozie_directory}/webrequest/legacy_tsvs/workflow.xml

# HDFS path to webrequest dataset definition
webrequest_datasets_file               = ${oozie_directory}/webrequest/datasets.xml

# Time to start running this coordinator.
# Make sure to have hours and minutes at 0!
start_time                             = 2014-04-01T00:00Z

# Time to stop running this coordinator.  Year 3000 == never!
stop_time                              = 3000-01-01T00:00Z

# HDFS path to workflow to mark a directory as done
mark_directory_done_workflow_file      = ${oozie_directory}/util/mark_directory_done/workflow.xml

archive_job_output_workflow_file       = ${oozie_directory}/util/archive_job_output/workflow.xml

# HDFS path to hive-site.xml file.  This is needed to run hive actions.
hive_site_xml                          = ${oozie_directory}/util/hive/hive-site.xml

# Table to write hourly pagecounts to (fully qualified)
webrequest_table                       = wmf.webrequest

# HDFS path to directory where webrequst data is time bucketed.
webrequest_data_directory              = ${name_node}/wmf/data/wmf/webrequest

# Temporary directory
temporary_directory                    = ${name_node}/tmp

# Archive base directory
archive_directory                      = ${name_node}/wmf/data/archive

# Archive directory for webrequest data
webrequest_archive_directory           = ${archive_directory}/webrequest

# Coordintator to start.
oozie.bundle.application.path          = ${oozie_directory}/webrequest/legacy_tsvs/bundle.xml
oozie.use.system.libpath               = true
oozie.action.external.stats.write      = true
